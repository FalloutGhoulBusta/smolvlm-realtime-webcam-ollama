# ğŸ“¹ SmolVLM Realtime Webcam [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> Real-time webcam interaction with vision language models via Ollama

## ğŸš€ Quick Start

1. **Install Ollama** and pull a vision model:
   ```bash
   ollama pull llava
   # or: ollama pull llava:7b, llava:13b, bakllava
   ```

2. Start Ollama with CORS enabled:
   ```bash
   set OLLAMA_ORIGINS=* && ollama serve
   ```

3. Serve the HTML file:
   ```bash
   python -m http.server 8000
   ```

4. Open [http://localhost:8000](http://localhost:8000) in your browser

## ğŸ¯ Features

- ğŸ“¸ **Live Webcam Capture** - Real-time video feed
- ğŸ¤– **AI Vision Analysis** - Describe what the camera sees
- âš¡ **Configurable Intervals** - 100ms to 2s between requests
- ğŸ”„ **Continuous Processing** - Start/stop with one click

## âš™ï¸ Configuration

- **Base API**: `http://localhost:11434` (Ollama default)
- **Model**: Must be a vision-capable model from Ollama library
  - âœ… `llava`, `llava:7b`, `llava:13b`, `bakllava`
  - âŒ Text-only models won't work
- **Instruction**: Customize what you want the AI to analyze

## ğŸ¤– Ollama Models Tested

- [moondream](https://ollama.com/library/moondream) - A vision model optimized for real-time analysis
- [hf.co/mradermacher/SmolLM2-135M-Instruct-GGUF:Q2_K](https://huggingface.co/mradermacher/SmolLM2-135M-Instruct-GGUF) - Lightweight instruct-tuned model for efficient processing

## ğŸ› ï¸ Requirements

- Modern browser with webcam access
- Ollama with a vision model installed
- Local HTTP server (Python, Node.js, etc.)

## ğŸ”§ Troubleshooting

- **CORS Error?** â†’ Start Ollama with `OLLAMA_ORIGINS=*`
- **Camera blocked?** â†’ Grant permissions and use HTTPS/localhost
- **No response?** â†’ Ensure you're using a vision model, not text-only

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Built for real-time AI vision interaction ğŸ¥âœ¨